{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T09:38:53.922624Z",
     "start_time": "2025-10-10T09:38:50.647958Z"
    }
   },
   "source": [
    "import torch\n",
    "from utils import DataManager, dataset_sizes, collect_training_data, compute_statistics, compute_average_accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "from probes import CCSProbe, TTPD, LRProbe, MMProbe, ALL_PROBES, TTPD_TYPES, measure_polarity_direction_lr, run_ray\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-10 10:38:53,435\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-10-10 10:38:53,490\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T09:39:04.024932Z",
     "start_time": "2025-10-10T09:39:04.002506Z"
    }
   },
   "source": [
    "# hyperparameters\n",
    "model_family = 'Llama3' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\n",
    "model_size = '8B'\n",
    "model_type = 'chat' # options are 'chat' or 'base'\n",
    "layer = 12 # layer from which to extract activations\n",
    "\n",
    "device = 'mps' if torch.mps.is_available() else 'cpu' # mps speeds up CCS training a fair bit but is not required\n",
    "device = \"cuda\" if torch.cuda.is_available() else device # cuda speeds it up a bit more\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T09:39:05.467680Z",
     "start_time": "2025-10-10T09:39:05.464551Z"
    }
   },
   "source": [
    "# define datasets used for training\n",
    "train_sets = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\", \"inventors\", \"neg_inventors\", \"animal_class\",\n",
    "                  \"neg_animal_class\", \"element_symb\", \"neg_element_symb\", \"facts\", \"neg_facts\"]\n",
    "\n",
    "# train_sets = [\"cities\", \"sp_en_trans\", \"inventors\",  \"animal_class\", \"element_symb\", \"facts\"]\n",
    "\n",
    "# get size of each training dataset to include an equal number of statements from each topic in training data\n",
    "train_set_sizes = dataset_sizes(train_sets)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parameter Hyper Optimization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=4, ignore_reinit_error=True)\n",
    "\n",
    "# Run optimization\n",
    "final_probe, best_config, analysis = run_ray()\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-10 10:39:12</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:03.09        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.4/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None<br>Logical resource usage: 1.0/4 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ttpd_with_cv_9d7f287a</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-10-10_10-39-07_663922_5540/artifacts/2025-10-10_10-39-09/ttpd_optimization/driver_artifacts/train_ttpd_with_cv_9d7f287a_1_features=proj_t_g_proj_t_p_proj_t_p_inter,final_C=0.0008,final_max_iter=2000,final_penalty=l2,final__2025-10-10_10-39-09/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th>features            </th><th style=\"text-align: right;\">    final_C</th><th style=\"text-align: right;\">  final_max_iter</th><th>final_penalty  </th><th>final_solver  </th><th style=\"text-align: right;\">  polarity_C</th><th style=\"text-align: right;\">  polarity_max_iter</th><th>polarity_penalty  </th><th>polarity_solver  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ttpd_with_cv_be717253</td><td>PENDING </td><td>              </td><td>[&#x27;proj_t_g&#x27;, &#x27;i_9f80</td><td style=\"text-align: right;\">0.0823906  </td><td style=\"text-align: right;\">            2000</td><td>l2             </td><td>liblinear     </td><td style=\"text-align: right;\">  0.00337608</td><td style=\"text-align: right;\">               5000</td><td>elasticnet        </td><td>saga             </td></tr>\n",
       "<tr><td>train_ttpd_with_cv_9d7f287a</td><td>ERROR   </td><td>127.0.0.1:5568</td><td>[&#x27;proj_t_g&#x27;, &#x27;p_c200</td><td style=\"text-align: right;\">0.000757603</td><td style=\"text-align: right;\">            2000</td><td>l2             </td><td>lbfgs         </td><td style=\"text-align: right;\">  5.24035   </td><td style=\"text-align: right;\">               2000</td><td>elasticnet        </td><td>liblinear        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 10:39:12,589\tERROR tune_controller.py:1331 -- Trial task failed for trial train_ttpd_with_cv_9d7f287a\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/_private/worker.py\", line 2882, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/_private/worker.py\", line 968, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001B[36mray::ImplicitFunc.train()\u001B[39m (pid=5568, ip=127.0.0.1, actor_id=b71ecd72ef0a3e13adc08d0b01000000, repr=train_ttpd_with_cv)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
      "    output = fn()\n",
      "             ^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/PycharmProjects/Truth_is_Universal/probes.py\", line 229, in train_ttpd_with_cv\n",
      "    probe = TTPDTestConfigurable.from_data(acts_centered, acts, labels, polarities, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/PycharmProjects/Truth_is_Universal/probes.py\", line 87, in from_data\n",
      "    probe.polarity_direc = learn_polarity_direction(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/PycharmProjects/Truth_is_Universal/probes.py\", line 33, in learn_polarity_direction\n",
      "    polarity_direc = polarity_direction_lr(acts, polarities, C=C, penalty=penalty,\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/PycharmProjects/Truth_is_Universal/probes.py\", line 28, in polarity_direction_lr\n",
      "    LR_polarity.fit(acts.numpy(), polarities_copy.numpy())\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "2025-10-10 10:39:12,699\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-10-10 10:39:12,703\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/flohop/ray_results/ttpd_optimization' in 0.0030s.\n",
      "2025-10-10 10:39:15,091\tERROR tune.py:1037 -- Trials did not complete: [train_ttpd_with_cv_9d7f287a]\n",
      "2025-10-10 10:39:15,092\tINFO tune.py:1041 -- Total run time: 5.49 seconds (3.09 seconds for the tuning loop).\n",
      "2025-10-10 10:39:15,092\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2025-10-10 10:39:15,100\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_ttpd_with_cv_be717253: FileNotFoundError('Could not fetch metrics for train_ttpd_with_cv_be717253: both result.json and progress.csv were not found at /Users/flohop/ray_results/ttpd_optimization/train_ttpd_with_cv_be717253_2_features=proj_t_g_inter1_inter2_inter3,final_C=0.0824,final_max_iter=2000,final_penalty=l2,final_sol_2025-10-10_10-39-12')\n",
      "2025-10-10 10:39:15,100\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
      "2025-10-10 10:39:15,101\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Best Trial: None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'last_result'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m ray.init(num_cpus=\u001B[32m4\u001B[39m, ignore_reinit_error=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Run optimization\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m final_probe, best_config, analysis = \u001B[43mrun_ray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Shutdown Ray\u001B[39;00m\n\u001B[32m      9\u001B[39m ray.shutdown()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Truth_is_Universal/probes.py:497\u001B[39m, in \u001B[36mrun_ray\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    480\u001B[39m analysis = optimize_ttpd_hyperparameters(\n\u001B[32m    481\u001B[39m     data_loader_fn=collect_training_data_tuner,\n\u001B[32m    482\u001B[39m     train_sets=train_sets,\n\u001B[32m   (...)\u001B[39m\u001B[32m    493\u001B[39m     use_gpu=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    494\u001B[39m )\n\u001B[32m    496\u001B[39m \u001B[38;5;66;03m# Analyze results\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m497\u001B[39m best_config, results_df = \u001B[43manalyze_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43manalysis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[38;5;66;03m# Train final model with best hyperparameters\u001B[39;00m\n\u001B[32m    500\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m80\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Truth_is_Universal/probes.py:424\u001B[39m, in \u001B[36manalyze_results\u001B[39m\u001B[34m(analysis)\u001B[39m\n\u001B[32m    422\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m80\u001B[39m)\n\u001B[32m    423\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mBest Trial: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_trial\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m424\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mbest_trial\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlast_result\u001B[49m[\u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33maccuracy_std\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m best_trial.last_result:\n\u001B[32m    427\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAccuracy Std Dev: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_trial.last_result[\u001B[33m'\u001B[39m\u001B[33maccuracy_std\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute 'last_result'"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Polarity direction accuracy"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T10:57:30.391632Z",
     "start_time": "2025-10-10T10:57:30.312501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\", \"sp_en_trans_disj\",\n",
    "                \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "                \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\",\n",
    "                \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "val_set_sizes = dataset_sizes(val_sets)\n",
    "\n",
    "cv_train_sets = np.array(train_sets)\n",
    "cv_test_sets = np.array(val_sets)\n",
    "acts_centered, acts, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family,\n",
    "                                                                    model_size, model_type, layer)\n",
    "\n",
    "# Test set\n",
    "t_acts_centered, t_acts, t_labels, t_polarities = collect_training_data(cv_test_sets, dataset_sizes(val_sets), model_family, model_size, model_type, layer)\n",
    "\n",
    "measure_polarity_direction_lr(acts, polarities, t_acts, t_polarities)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9920634920634921"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unseen topics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compare TTPD, LR and CCS on topic-specific datasets\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "num_iter = 3\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter * len(train_sets)\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            indices = np.arange(0, 12, 2)\n",
    "            for i in indices:\n",
    "                cv_train_sets = np.delete(np.array(train_sets), [i, i+1], axis=0)\n",
    "                # load training data\n",
    "                acts_centered, acts, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family,\n",
    "                                                                                model_size, model_type, layer)\n",
    "\n",
    "                if probe_type in TTPD_CLASSES:\n",
    "                    probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "                elif probe_type == LRProbe:\n",
    "                    probe = LRProbe.from_data(acts, labels)\n",
    "                elif probe_type == CCSProbe:\n",
    "                    acts_affirm = acts[polarities == 1.0]\n",
    "                    acts_neg = acts[polarities == -1.0]\n",
    "                    labels_affirm = labels[polarities == 1.0]\n",
    "                    mean_affirm = torch.mean(acts_affirm, dim=0)\n",
    "                    mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                    acts_affirm = acts_affirm - mean_affirm\n",
    "                    acts_neg = acts_neg - mean_neg\n",
    "                    probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "                elif probe_type == MMProbe:\n",
    "                    probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "                # evaluate classification accuracy on held out datasets\n",
    "                dm = DataManager()\n",
    "                for j in range(0,2):\n",
    "                    dm.add_dataset(train_sets[i+j], model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                    acts, labels = dm.data[train_sets[i+j]]\n",
    "\n",
    "                    # classifier specific predictions\n",
    "                    if probe_type == CCSProbe:\n",
    "                        if j == 0:\n",
    "                            acts = acts - mean_affirm\n",
    "                        if j == 1:\n",
    "                            acts = acts - mean_neg\n",
    "                    predictions = probe.pred(acts)\n",
    "                    results[probe_type][train_sets[i+j]].append((predictions == labels).float().mean().item())\n",
    "                    pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "probes = [p for (t, p ) in ALL_PROBES]\n",
    "titles = [t for (t, p) in ALL_PROBES]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6), ncols=len(probes))\n",
    "\n",
    "if len(probes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "\n",
    "for t, (ax, key) in enumerate(zip(axes, probes)):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in train_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in train_sets]\n",
    "\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}',\n",
    "                    ha='center', va='center', fontsize=13)\n",
    "\n",
    "    ax.set_yticks(range(len(train_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=12)\n",
    "\n",
    "# y tick labels only on first subplot\n",
    "axes[0].set_yticklabels(train_sets, fontsize=12)\n",
    "for ax in axes[1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6, location=\"right\")\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation to logical conjunctions and disjunctions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compare TTPD, LR, CCS and MM on logical conjunctions and disjunctions\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\",\"sp_en_trans_disj\",\n",
    "             \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "               \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\",\n",
    "            \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n",
    "\n",
    "\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                             model_type, layer)\n",
    "\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set] # retrieve the activations and labels that were just added to the DM\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts) # one prediction per example. 0 if we think its a lie, 1 if we predicte its true\n",
    "\n",
    "                # compare prediction with ground truth labels and average it\n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "probes = [p for (t, p ) in ALL_PROBES]\n",
    "titles = [t for (t, p) in ALL_PROBES]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6), ncols=len(probes))\n",
    "\n",
    "if len(probes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "\n",
    "for t, (ax, key) in enumerate(zip(axes, probes)):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in val_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in val_sets]\n",
    "\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}',\n",
    "                    ha='center', va='center', fontsize=13)\n",
    "\n",
    "    ax.set_yticks(range(len(val_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=12)\n",
    "\n",
    "# y tick labels only on first subplot\n",
    "axes[0].set_yticklabels(val_sets, fontsize=12)\n",
    "for ax in axes[1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6, location='right')\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation to German statements"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compare TTPD, LR, CCS and MM on statements translated to german\n",
    "val_sets = [\"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "                  \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\"]\n",
    "\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                                           model_type, layer)\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                \n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probes = [p for (t, p ) in ALL_PROBES]\n",
    "titles = [t for (t, p) in ALL_PROBES]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6), ncols=len(probes))\n",
    "\n",
    "\n",
    "if len(probes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for t, (ax, key) in enumerate(zip(axes, probes)):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in val_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in val_sets]\n",
    "\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}',\n",
    "                    ha='center', va='center', fontsize=13)\n",
    "\n",
    "    ax.set_yticks(range(len(val_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=12)\n",
    "\n",
    "# y tick labels only on first subplot\n",
    "axes[0].set_yticklabels(val_sets, fontsize=12)\n",
    "for ax in axes[1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6, location='right')\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying generalisation to Conjunctions, Disjunctions and German statements in one table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the validation sets and the probe types\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\",\"sp_en_trans_disj\",\n",
    "             \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "               \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\", \"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "                  \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\",\n",
    "            \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "num_iter = 20\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n",
    "\n",
    "# Training and evaluating classifiers\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar:\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                                           model_type, layer)\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the groups\n",
    "groups = {\n",
    "    'Conjunctions': [dataset for dataset in val_sets if dataset.endswith('_conj')],\n",
    "    'Disjunctions': [dataset for dataset in val_sets if dataset.endswith('_disj')],\n",
    "    'Affirmative German': [dataset for dataset in val_sets if dataset.endswith('_de') and not dataset.startswith('neg_')],\n",
    "    'Negated German': [dataset for dataset in val_sets if dataset.startswith('neg_') and dataset.endswith('_de')],\n",
    "    'common_claim_true_false': ['common_claim_true_false'],\n",
    "    'counterfact_true_false': ['counterfact_true_false']\n",
    "}\n",
    "\n",
    "# Initialize group results\n",
    "group_results = {probe_type: {group_name: [] for group_name in groups} for probe_type in probe_types}\n",
    "\n",
    "# Process results to compute mean accuracies per group per classifier\n",
    "for probe_type in probe_types:\n",
    "    for n in range(num_iter):\n",
    "        for group_name, group_datasets in groups.items():\n",
    "            accuracies = []\n",
    "            for dataset in group_datasets:\n",
    "                accuracy = results[probe_type][dataset][n]\n",
    "                accuracies.append(accuracy)\n",
    "            mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "            group_results[probe_type][group_name].append(mean_accuracy)\n",
    "\n",
    "# Compute statistics\n",
    "stat_group_results = {probe_type: {'mean': {}, 'std': {}} for probe_type in probe_types}\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    for group_name in groups:\n",
    "        accuracies = group_results[probe_type][group_name]\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        stat_group_results[probe_type]['mean'][group_name] = mean_accuracy\n",
    "        stat_group_results[probe_type]['std'][group_name] = std_accuracy\n",
    "\n",
    "# Map probe types to classifier names\n",
    "\n",
    "probe_type_to_name = {probe:name for (name, probe) in ALL_PROBES}\n",
    "\n",
    "# probe_type_to_name = {\n",
    "#     TTPD: 'TTPD',\n",
    "#     TTPD4d: \"TTPD4d\",\n",
    "#     TTPD3dTp: \"TTPD3dTp\",\n",
    "#     TTPD3dTpInv: \"TTPD3dTpInv\",\n",
    "#     LRProbe: 'LR',\n",
    "#     CCSProbe: 'CCS',\n",
    "#     MMProbe: 'MM'\n",
    "# }\n",
    "\n",
    "# Create DataFrames for mean accuracies and standard deviations\n",
    "group_names = ['Conjunctions', 'Disjunctions', 'Affirmative German', 'Negated German', 'common_claim_true_false', 'counterfact_true_false']\n",
    "classifier_names = [n for (n, _) in ALL_PROBES]\n",
    "# classifier_names = ['TTPD', \"TTPD4d\", \"TTPD3dTp\", \"TTPD3dTpInv\", 'LR', 'CCS', 'MM']\n",
    "\n",
    "mean_df = pd.DataFrame(index=group_names, columns=classifier_names)\n",
    "std_df = pd.DataFrame(index=group_names, columns=classifier_names)\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    classifier_name = probe_type_to_name[probe_type]\n",
    "    for group_name in group_names:\n",
    "        mean_accuracy = stat_group_results[probe_type]['mean'][group_name]\n",
    "        std_accuracy = stat_group_results[probe_type]['std'][group_name]\n",
    "        mean_df.loc[group_name, classifier_name] = mean_accuracy\n",
    "        std_df.loc[group_name, classifier_name] = std_accuracy\n",
    "\n",
    "num_classifiers = len(classifier_names)\n",
    "fig, axes = plt.subplots(figsize=(2.5*num_classifiers, 6), ncols=num_classifiers)\n",
    "\n",
    "for idx, classifier_name in enumerate(classifier_names):\n",
    "    ax = axes[idx]\n",
    "    mean_values = mean_df[classifier_name].values.astype(float)\n",
    "    std_values = std_df[classifier_name].values.astype(float)\n",
    "\n",
    "    # Create heatmap with a single column\n",
    "    im = ax.imshow(mean_values[:, np.newaxis], vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    # Annotate the heatmap\n",
    "    for i in range(len(group_names)):\n",
    "        mean_accuracy = mean_values[i]\n",
    "        std_accuracy = std_values[i]\n",
    "        ax.text(0, i, f'{round(mean_accuracy * 100):2d} ± {round(std_accuracy * 100):2d}',\n",
    "                ha='center', va='center', fontsize=14)\n",
    "\n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    if idx == 0:\n",
    "        ax.set_yticks(np.arange(len(group_names)))\n",
    "        ax.set_yticklabels(group_names, fontsize=14)\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    ax.set_title(classifier_name, fontsize=15)\n",
    "\n",
    "# Add colorbar on the right\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), location='right', shrink=0.8)\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Classification Accuracies\", fontsize=17)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world scenarios / lies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: [] for t in probe_types}\n",
    "num_iter = 50\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family,\n",
    "                                                                                           model_size, model_type,layer)\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on real world scenarios\n",
    "            dm = DataManager()\n",
    "            real_world_dataset = \"real_world_scenarios/all_unambiguous_replies\"\n",
    "            dm.add_dataset(real_world_dataset, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "            acts, labels = dm.data[real_world_dataset]\n",
    "            \n",
    "            # classifier specific predictions\n",
    "            if probe_type == CCSProbe:\n",
    "                acts = acts - (mean_affirm + mean_neg)/2\n",
    "\n",
    "            predictions = probe.pred(acts)\n",
    "            results[probe_type].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    mean = np.mean(results[probe_type])\n",
    "    std = np.std(results[probe_type])\n",
    "    print(f\"{probe_type.__name__}:\")\n",
    "    print(f\"  Mean Accuracy: {mean*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation: {std*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
