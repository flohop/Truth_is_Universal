{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T15:59:28.461181Z",
     "start_time": "2025-10-13T15:59:28.455343Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    ")\n",
    "from utils import load_hooked, get_svd\n",
    "import configparser\n",
    "\n",
    "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T15:59:29.456195Z",
     "start_time": "2025-10-13T15:59:29.438278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'mps' if torch.mps.is_available() else 'cpu' # mps speeds up CCS training a fair bit but is not required\n",
    "device = \"cuda\" if torch.cuda.is_available() else device # cuda speeds it up a bit more\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "model_family = 'Llama3' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\n",
    "model_size = '8B'\n",
    "model_type = 'chat' # options are 'chat' or 'base'\n",
    "device"
   ],
   "id": "afa8d8a5d98e3e4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T15:59:31.980026Z",
     "start_time": "2025-10-13T15:59:31.272495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import ROOT_DIR\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"checkpoints\")\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "model_path = os.path.join(config[model_family]['weights_directory'],\n",
    "                          config[model_family][f'{model_size}_{model_type}_subdir'],\n",
    "                          \"original\", \"consolidated.00.pth\")\n",
    "\n",
    "model = load_hooked(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    weights_path=model_path,\n",
    ")\n",
    "\n",
    "gpt2 = HookedTransformer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", device=device)\n",
    "gpt2.tokenizer.padding_side = \"left\"\n",
    "gpt2.tokenizer.pad_token_id = gpt2.tokenizer.eos_token_id\n",
    "\n",
    "\n"
   ],
   "id": "b5e27412f0cb5476",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 27.96 GiB",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      7\u001B[39m config.read(\u001B[33m'\u001B[39m\u001B[33mconfig.ini\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      8\u001B[39m model_path = os.path.join(config[model_family][\u001B[33m'\u001B[39m\u001B[33mweights_directory\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m      9\u001B[39m                           config[model_family][\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_subdir\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     10\u001B[39m                           \u001B[33m\"\u001B[39m\u001B[33moriginal\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mconsolidated.00.pth\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m model = \u001B[43mload_hooked\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmeta-llama/Meta-Llama-3-8B\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweights_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m gpt2 = HookedTransformer.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mmeta-llama/Meta-Llama-3-8B\u001B[39m\u001B[33m\"\u001B[39m, device=device)\n\u001B[32m     18\u001B[39m gpt2.tokenizer.padding_side = \u001B[33m\"\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Truth_is_Universal/utils.py:392\u001B[39m, in \u001B[36mload_hooked\u001B[39m\u001B[34m(model_name, weights_path)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_hooked\u001B[39m(model_name, weights_path):\n\u001B[32m--> \u001B[39m\u001B[32m392\u001B[39m     _model = \u001B[43mHookedTransformer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m                                               \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m                                               \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    395\u001B[39m     cfg = _model.cfg\n\u001B[32m    398\u001B[39m     _weights = t.load(weights_path, map_location=t.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))[\n\u001B[32m    399\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mstate\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    400\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1370\u001B[39m, in \u001B[36mHookedTransformer.from_pretrained\u001B[39m\u001B[34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001B[39m\n\u001B[32m   1366\u001B[39m     center_unembed = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   1368\u001B[39m \u001B[38;5;66;03m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\u001B[39;00m\n\u001B[32m   1369\u001B[39m \u001B[38;5;66;03m# match the HookedTransformer parameter names.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1370\u001B[39m state_dict = \u001B[43mloading\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_pretrained_state_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhf_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfrom_pretrained_kwargs\u001B[49m\n\u001B[32m   1372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1374\u001B[39m \u001B[38;5;66;03m# Create the HookedTransformer object\u001B[39;00m\n\u001B[32m   1375\u001B[39m model = \u001B[38;5;28mcls\u001B[39m(\n\u001B[32m   1376\u001B[39m     cfg,\n\u001B[32m   1377\u001B[39m     tokenizer,\n\u001B[32m   1378\u001B[39m     move_to_device=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1379\u001B[39m     default_padding_side=default_padding_side,\n\u001B[32m   1380\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:1939\u001B[39m, in \u001B[36mget_pretrained_state_dict\u001B[39m\u001B[34m(official_model_name, cfg, hf_model, dtype, **kwargs)\u001B[39m\n\u001B[32m   1932\u001B[39m         hf_model = T5ForConditionalGeneration.from_pretrained(\n\u001B[32m   1933\u001B[39m             official_model_name,\n\u001B[32m   1934\u001B[39m             torch_dtype=dtype,\n\u001B[32m   1935\u001B[39m             token=huggingface_token \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(huggingface_token) > \u001B[32m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1936\u001B[39m             **kwargs,\n\u001B[32m   1937\u001B[39m         )\n\u001B[32m   1938\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1939\u001B[39m         hf_model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1940\u001B[39m \u001B[43m            \u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1941\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1942\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhuggingface_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhuggingface_token\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m>\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1943\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1944\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1946\u001B[39m     \u001B[38;5;66;03m# Load model weights, and fold in layer norm weights\u001B[39;00m\n\u001B[32m   1948\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m hf_model.parameters():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    602\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    603\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    606\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    608\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    609\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    610\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001B[39m, in \u001B[36mrestore_default_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    275\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    276\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    279\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:5051\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   5041\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   5042\u001B[39m         torch.set_default_dtype(dtype_orig)\n\u001B[32m   5044\u001B[39m     (\n\u001B[32m   5045\u001B[39m         model,\n\u001B[32m   5046\u001B[39m         missing_keys,\n\u001B[32m   5047\u001B[39m         unexpected_keys,\n\u001B[32m   5048\u001B[39m         mismatched_keys,\n\u001B[32m   5049\u001B[39m         offload_index,\n\u001B[32m   5050\u001B[39m         error_msgs,\n\u001B[32m-> \u001B[39m\u001B[32m5051\u001B[39m     ) = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5052\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5053\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5054\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5055\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5056\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5057\u001B[39m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5058\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5059\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5060\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5061\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5062\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5063\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5064\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5065\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5066\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5067\u001B[39m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[32m   5068\u001B[39m model.tie_weights()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:5435\u001B[39m, in \u001B[36mPreTrainedModel._load_pretrained_model\u001B[39m\u001B[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[39m\n\u001B[32m   5433\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_hqq_or_quark:\n\u001B[32m   5434\u001B[39m     expanded_device_map = expand_device_map(device_map, expected_keys)\n\u001B[32m-> \u001B[39m\u001B[32m5435\u001B[39m     \u001B[43mcaching_allocator_warmup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpanded_device_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5437\u001B[39m \u001B[38;5;66;03m# Prepare and compatabilize arguments for serial and parallel shard loading\u001B[39;00m\n\u001B[32m   5438\u001B[39m args_list = [\n\u001B[32m   5439\u001B[39m     (\n\u001B[32m   5440\u001B[39m         shard_file,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5455\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m shard_file \u001B[38;5;129;01min\u001B[39;00m checkpoint_files\n\u001B[32m   5456\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:6108\u001B[39m, in \u001B[36mcaching_allocator_warmup\u001B[39m\u001B[34m(model, expanded_device_map, hf_quantizer)\u001B[39m\n\u001B[32m   6106\u001B[39m     byte_count = \u001B[38;5;28mmax\u001B[39m(\u001B[32m0\u001B[39m, byte_count - unused_memory)\n\u001B[32m   6107\u001B[39m \u001B[38;5;66;03m# Allocate memory\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m6108\u001B[39m _ = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbyte_count\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mfactor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequires_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/utils/_device.py:103\u001B[39m, in \u001B[36mDeviceContext.__torch_function__\u001B[39m\u001B[34m(self, func, types, args, kwargs)\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m _device_constructors() \u001B[38;5;129;01mand\u001B[39;00m kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    102\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.device\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Invalid buffer size: 27.96 GiB"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
