{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "7257b6bf815a5881"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Causal Intervention to Test for a Shortcut\n",
    "This notebook tests the hypothesis that a specific direction in activation space causally influences the model's truthfulness. We will:\n",
    "1. **Calculate the 'shortcut vector'** by finding the difference between mean true and mean false activations at a late layer.\n",
    "2. **Intervene** during a forward pass by adding or subtracting this vector.\n",
    "3. **Observe** if the intervention flips the model's output from true to false, or vice versa."
   ],
   "id": "ee9bb2488abe85b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T12:34:58.830696Z",
     "start_time": "2025-10-15T12:34:54.976572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch as t\n",
    "\n",
    "# Import our custom functions and classes\n",
    "from utils import DataManager, load_model\n",
    "from intervention import run_intervention_experiment\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_FAMILY = \"Llama3\"\n",
    "MODEL_SIZE = \"8B\"\n",
    "MODEL_TYPE = \"base\"\n",
    "TARGET_LAYER = 22 # A layer in the identified 20-26 range\n",
    "DEVICE = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "DEVICE = \"mps\" if t.mps.is_available() else DEVICE\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "fda6d5619169ffd1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Calculate the Shortcut Vector",
   "id": "88641fea34fed231"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T12:35:02.713520Z",
     "start_time": "2025-10-15T12:35:02.594505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Calculating the shortcut vector for Layer {TARGET_LAYER}...\")\n",
    "\n",
    "# Load pre-computed activations using the DataManager\n",
    "dm = DataManager()\n",
    "dm.add_dataset('cities', MODEL_FAMILY, MODEL_SIZE, MODEL_TYPE, TARGET_LAYER, center=False, device=DEVICE)\n",
    "dm.add_dataset('neg_cities', MODEL_FAMILY, MODEL_SIZE, MODEL_TYPE, TARGET_LAYER, center=False, device=DEVICE)\n",
    "\n",
    "# --- CORRECTED LOGIC ---\n",
    "# Get activations and labels from both datasets\n",
    "cities_acts, cities_labels = dm.data['cities']\n",
    "neg_cities_acts, neg_cities_labels = dm.data['neg_cities']\n",
    "\n",
    "# 1. Collect all TRUE activations (label == 1) from BOTH datasets\n",
    "true_acts_from_cities = cities_acts[cities_labels == 1]\n",
    "true_acts_from_neg_cities = neg_cities_acts[neg_cities_labels == 1]\n",
    "all_true_acts = t.cat([true_acts_from_cities, true_acts_from_neg_cities], dim=0)\n",
    "\n",
    "# 2. Collect all FALSE activations (label == 0) from BOTH datasets\n",
    "false_acts_from_cities = cities_acts[cities_labels == 0]\n",
    "false_acts_from_neg_cities = neg_cities_acts[neg_cities_labels == 0]\n",
    "all_false_acts = t.cat([false_acts_from_cities, false_acts_from_neg_cities], dim=0)\n",
    "\n",
    "print(f\"Found {len(all_true_acts)} total true statements.\")\n",
    "print(f\"Found {len(all_false_acts)} total false statements.\")\n",
    "\n",
    "# 3. Calculate means based on the correctly aggregated groups\n",
    "mean_true_acts = all_true_acts.mean(dim=0)\n",
    "mean_false_acts = all_false_acts.mean(dim=0)\n",
    "\n",
    "# This vector points from the \"average lie\" to the \"average truth\"\n",
    "shortcut_vector = mean_true_acts - mean_false_acts\n",
    "\n",
    "print(f\"Shortcut vector calculated. Norm: {shortcut_vector.norm().item():.2f}\")"
   ],
   "id": "31cb3c482845eb1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the shortcut vector for Layer 22...\n",
      "Found 45 total true statements.\n",
      "Found 45 total false statements.\n",
      "Shortcut vector calculated. Norm: 3.62\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2: Load Model and Run Intervention Experiments",
   "id": "6efee4aa5b69b04f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T12:40:45.027608Z",
     "start_time": "2025-10-15T12:39:32.758478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loading model...\")\n",
    "tokenizer, model = load_model(MODEL_FAMILY, MODEL_SIZE, MODEL_TYPE, DEVICE)\n",
    "\n",
    "# --- Experiment 1: Try to flip a TRUE statement to FALSE ---\n",
    "test_statement_true = \"The Eiffel Tower is in the city of\" # Model should complete 'Paris'\n",
    "# We subtract the vector (negative alpha) to push it from the 'truth' region towards the 'lie' region\n",
    "alpha_for_falsehood = -4.0\n",
    "run_intervention_experiment(model, tokenizer, shortcut_vector, test_statement_true, TARGET_LAYER, alpha=alpha_for_falsehood)\n",
    "\n",
    "# --- Experiment 2: Try to flip a FALSE statement to TRUE ---\n",
    "# Let's set up a false context. The model 'knows' the capital of Italy is Rome.\n",
    "test_statement_false = \"The capital of Italy is the city of London, not the city of\"\n",
    "# We add the vector to push its state towards 'truth', hoping it corrects itself to 'Rome'\n",
    "alpha_for_truth = 4.0\n",
    "run_intervention_experiment(model, tokenizer, shortcut_vector, test_statement_false, TARGET_LAYER, alpha=alpha_for_truth)"
   ],
   "id": "efb7db972fc24106",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f419b451b20943938d1e47bd7c8657d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 36.28 GiB, other allocations: 800.00 KiB, max allowed: 36.27 GiB). Tried to allocate 8.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mLoading model...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m tokenizer, model = \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODEL_FAMILY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMODEL_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMODEL_TYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# --- Experiment 1: Try to flip a TRUE statement to FALSE ---\u001B[39;00m\n\u001B[32m      5\u001B[39m test_statement_true = \u001B[33m\"\u001B[39m\u001B[33mThe Eiffel Tower is in the city of\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;66;03m# Model should complete 'Paris'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Truth_is_Universal/utils.py:50\u001B[39m, in \u001B[36mload_model\u001B[39m\u001B[34m(model_family, model_size, model_type, device)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     48\u001B[39m     model = model.half()\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer, \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:4347\u001B[39m, in \u001B[36mPreTrainedModel.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   4342\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[32m   4343\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   4344\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4345\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m `dtype` by passing the correct `dtype` argument.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4346\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m4347\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/nn/modules/module.py:1369\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1366\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1367\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1369\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/nn/modules/module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/nn/modules/module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 928 (2 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/nn/modules/module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/nn/modules/module.py:955\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    951\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    952\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m955\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    956\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    958\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1348\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1349\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1350\u001B[39m             device,\n\u001B[32m   1351\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1352\u001B[39m             non_blocking,\n\u001B[32m   1353\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1354\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1355\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1356\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1357\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1361\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 36.28 GiB, other allocations: 800.00 KiB, max allowed: 36.27 GiB). Tried to allocate 8.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ea66c12aa528ea58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
