{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T15:40:14.108470Z",
     "start_time": "2025-10-13T15:40:11.212266Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    ")\n",
    "from utils import load_hooked, get_svd\n",
    "import configparser\n",
    "\n",
    "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/flohop/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T15:40:16.375981Z",
     "start_time": "2025-10-13T15:40:16.360998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'mps' if torch.mps.is_available() else 'cpu' # mps speeds up CCS training a fair bit but is not required\n",
    "device = \"cuda\" if torch.cuda.is_available() else device # cuda speeds it up a bit more\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "model_family = 'Llama3' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\n",
    "model_size = '8B'\n",
    "model_type = 'chat' # options are 'chat' or 'base'\n",
    "device"
   ],
   "id": "afa8d8a5d98e3e4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T15:43:25.819701Z",
     "start_time": "2025-10-13T15:42:42.163656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"checkpoints\")\n",
    "\n",
    "model = load_hooked(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    weights_path=os.path.join(MODEL_DIR, \"dpo.pt\"),\n",
    ")\n",
    "\n",
    "gpt2 = HookedTransformer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", device=device)\n",
    "gpt2.tokenizer.padding_side = \"left\"\n",
    "gpt2.tokenizer.pad_token_id = gpt2.tokenizer.eos_token_id\n",
    "\n",
    "\n"
   ],
   "id": "b5e27412f0cb5476",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'checkpoint_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      3\u001B[39m config = configparser.ConfigParser()\n\u001B[32m      4\u001B[39m config.read(\u001B[33m\"\u001B[39m\u001B[33mconfig.ini\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m tokenizer, model = \u001B[43mload_model_interpretable\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_family\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmeta-llama/Meta-Llama-3-8B\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_size\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m8B\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43moriginal\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# or \"cuda\"\u001B[39;49;00m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Truth_is_Universal/generate_acts.py:35\u001B[39m, in \u001B[36mload_model_interpretable\u001B[39m\u001B[34m(model_family, model_size, model_type, device)\u001B[39m\n\u001B[32m     32\u001B[39m     dtype = t.float16  \u001B[38;5;66;03m# can use float32 on CPU\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# Load HookedTransformer from local safetensors directory\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m model = \u001B[43mHookedTransformer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_family\u001B[49m\u001B[43m,\u001B[49m\u001B[43m       \u001B[49m\u001B[38;5;66;43;03m# can also use official HF name like \"meta-llama/Meta-Llama-3-8B\"\u001B[39;49;00m\n\u001B[32m     37\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# your local directory with safetensors + config.json\u001B[39;49;00m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# The tokenizer comes pre-attached\u001B[39;00m\n\u001B[32m     43\u001B[39m tokenizer = model.tokenizer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1370\u001B[39m, in \u001B[36mHookedTransformer.from_pretrained\u001B[39m\u001B[34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001B[39m\n\u001B[32m   1366\u001B[39m     center_unembed = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   1368\u001B[39m \u001B[38;5;66;03m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\u001B[39;00m\n\u001B[32m   1369\u001B[39m \u001B[38;5;66;03m# match the HookedTransformer parameter names.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1370\u001B[39m state_dict = \u001B[43mloading\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_pretrained_state_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhf_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfrom_pretrained_kwargs\u001B[49m\n\u001B[32m   1372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1374\u001B[39m \u001B[38;5;66;03m# Create the HookedTransformer object\u001B[39;00m\n\u001B[32m   1375\u001B[39m model = \u001B[38;5;28mcls\u001B[39m(\n\u001B[32m   1376\u001B[39m     cfg,\n\u001B[32m   1377\u001B[39m     tokenizer,\n\u001B[32m   1378\u001B[39m     move_to_device=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1379\u001B[39m     default_padding_side=default_padding_side,\n\u001B[32m   1380\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:1939\u001B[39m, in \u001B[36mget_pretrained_state_dict\u001B[39m\u001B[34m(official_model_name, cfg, hf_model, dtype, **kwargs)\u001B[39m\n\u001B[32m   1932\u001B[39m         hf_model = T5ForConditionalGeneration.from_pretrained(\n\u001B[32m   1933\u001B[39m             official_model_name,\n\u001B[32m   1934\u001B[39m             torch_dtype=dtype,\n\u001B[32m   1935\u001B[39m             token=huggingface_token \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(huggingface_token) > \u001B[32m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1936\u001B[39m             **kwargs,\n\u001B[32m   1937\u001B[39m         )\n\u001B[32m   1938\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1939\u001B[39m         hf_model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1940\u001B[39m \u001B[43m            \u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1941\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1942\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhuggingface_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhuggingface_token\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m>\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1943\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1944\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1946\u001B[39m     \u001B[38;5;66;03m# Load model weights, and fold in layer norm weights\u001B[39;00m\n\u001B[32m   1948\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m hf_model.parameters():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    602\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    603\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    606\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    608\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    609\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    610\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001B[39m, in \u001B[36mrestore_default_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    275\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    276\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    279\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/truth_is_universal/lib/python3.11/site-packages/transformers/modeling_utils.py:4974\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4971\u001B[39m config = copy.deepcopy(config)  \u001B[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001B[39;00m\n\u001B[32m   4972\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(model_init_context):\n\u001B[32m   4973\u001B[39m     \u001B[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4974\u001B[39m     model = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4976\u001B[39m \u001B[38;5;66;03m# Make sure to tie the weights correctly\u001B[39;00m\n\u001B[32m   4977\u001B[39m model.tie_weights()\n",
      "\u001B[31mTypeError\u001B[39m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'checkpoint_path'"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
