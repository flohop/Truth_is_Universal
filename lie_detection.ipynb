{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T17:21:41.165124Z",
     "start_time": "2025-10-10T17:21:37.909697Z"
    }
   },
   "source": [
    "import torch\n",
    "from utils import DataManager, dataset_sizes, collect_training_data, compute_statistics, compute_average_accuracies, plot_lr_feature_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from probes import CCSProbe, LRProbe, MMProbe, ALL_PROBES, TTPD_TYPES, measure_polarity_direction_lr, run_ray, get_average_coef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T17:21:42.173418Z",
     "start_time": "2025-10-10T17:21:42.157610Z"
    }
   },
   "source": [
    "# hyperparameters\n",
    "model_family = 'Llama3' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\n",
    "model_size = '8B'\n",
    "model_type = 'chat' # options are 'chat' or 'base'\n",
    "layer = 12 # layer from which to extract activations\n",
    "\n",
    "device = 'mps' if torch.mps.is_available() else 'cpu' # mps speeds up CCS training a fair bit but is not required\n",
    "device = \"cuda\" if torch.cuda.is_available() else device # cuda speeds it up a bit more\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T17:21:43.361836Z",
     "start_time": "2025-10-10T17:21:43.359031Z"
    }
   },
   "source": [
    "# define datasets used for training\n",
    "train_sets = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\", \"inventors\", \"neg_inventors\", \"animal_class\",\n",
    "                  \"neg_animal_class\", \"element_symb\", \"neg_element_symb\", \"facts\", \"neg_facts\"]\n",
    "\n",
    "# train_sets = [\"cities\", \"sp_en_trans\", \"inventors\",  \"animal_class\", \"element_symb\", \"facts\"]\n",
    "\n",
    "# get size of each training dataset to include an equal number of statements from each topic in training data\n",
    "train_set_sizes = dataset_sizes(train_sets)\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parameter Hyper Optimization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=4, ignore_reinit_error=True)\n",
    "\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\", \"sp_en_trans_disj\",\n",
    "                \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "                \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\",\n",
    "                \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "# Run optimization\n",
    "final_probe, best_config, analysis = run_ray(train_sets, val_sets)\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-10 18:22:03</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:16.54        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.0/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 96.000: None | Iter 48.000: None | Iter 24.000: None | Iter 12.000: 0.7241379022598267 | Iter 6.000: 1.0 | Iter 3.000: 1.0<br>Logical resource usage: 1.0/4 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th>features            </th><th style=\"text-align: right;\">    final_C</th><th>final_combo/penalty  </th><th>final_combo/solver  </th><th style=\"text-align: right;\">  final_l1_ratio</th><th style=\"text-align: right;\">  final_max_iter</th><th style=\"text-align: right;\">  polarity_C</th><th>polarity_combo/penal\n",
       "ty   </th><th>polarity_combo/solve\n",
       "r          </th><th style=\"text-align: right;\">  polarity_l1_ratio</th><th style=\"text-align: right;\">  polarity_max_iter</th><th>use_scaler  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  accuracy_std</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ttpd_with_cv_8606f3c5</td><td>PENDING   </td><td>               </td><td>[&#x27;proj_t_g&#x27;, &#x27;p_3540</td><td style=\"text-align: right;\">0.316366   </td><td>l1                   </td><td>liblinear           </td><td style=\"text-align: right;\">        0.341033</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">  0.00702554</td><td>l1</td><td>saga     </td><td style=\"text-align: right;\">           0.269722</td><td style=\"text-align: right;\">               5000</td><td>False       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>train_ttpd_with_cv_45c0bf76</td><td>TERMINATED</td><td>127.0.0.1:12810</td><td>[&#x27;proj_t_g&#x27;, &#x27;p_a1c0</td><td style=\"text-align: right;\">0.263993   </td><td>l1                   </td><td>liblinear           </td><td style=\"text-align: right;\">        0.818001</td><td style=\"text-align: right;\">            1000</td><td style=\"text-align: right;\">  0.0423506 </td><td>l2</td><td>lbfgs    </td><td style=\"text-align: right;\">           0.43874 </td><td style=\"text-align: right;\">               3000</td><td>True        </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">        0.127828</td><td style=\"text-align: right;\">  0.950263</td><td style=\"text-align: right;\">     0.0856619</td><td style=\"text-align: right;\">0.0497373</td></tr>\n",
       "<tr><td>train_ttpd_with_cv_7c461e90</td><td>TERMINATED</td><td>127.0.0.1:12813</td><td>[&#x27;proj_t_g&#x27;, &#x27;p_c880</td><td style=\"text-align: right;\">0.000828578</td><td>                     </td><td>lbfgs               </td><td style=\"text-align: right;\">        0.192403</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">  0.0009304 </td><td>l1</td><td>saga     </td><td style=\"text-align: right;\">           0.672523</td><td style=\"text-align: right;\">               5000</td><td>False       </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">        0.103346</td><td style=\"text-align: right;\">  0.931189</td><td style=\"text-align: right;\">     0.0968422</td><td style=\"text-align: right;\">0.0688114</td></tr>\n",
       "<tr><td>train_ttpd_with_cv_6c5f04fe</td><td>TERMINATED</td><td>127.0.0.1:12814</td><td>[&#x27;proj_t_g&#x27;, &#x27;p_a440</td><td style=\"text-align: right;\">0.344553   </td><td>l1                   </td><td>liblinear           </td><td style=\"text-align: right;\">        0.702185</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">  0.0108945 </td><td>l2</td><td>liblinear</td><td style=\"text-align: right;\">           0.774081</td><td style=\"text-align: right;\">               1000</td><td>False       </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">        0.151488</td><td style=\"text-align: right;\">  0.956175</td><td style=\"text-align: right;\">     0.0588715</td><td style=\"text-align: right;\">0.043825 </td></tr>\n",
       "<tr><td>train_ttpd_with_cv_02e153fd</td><td>TERMINATED</td><td>127.0.0.1:12815</td><td>[&#x27;proj_t_g&#x27;, &#x27;p_1c40</td><td style=\"text-align: right;\">0.00272752 </td><td>l2                   </td><td>liblinear           </td><td style=\"text-align: right;\">        0.407002</td><td style=\"text-align: right;\">            2000</td><td style=\"text-align: right;\">  2.04698   </td><td>l2</td><td>lbfgs    </td><td style=\"text-align: right;\">           0.472824</td><td style=\"text-align: right;\">               1000</td><td>False       </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">        0.14701 </td><td style=\"text-align: right;\">  0.93229 </td><td style=\"text-align: right;\">     0.0850108</td><td style=\"text-align: right;\">0.0677103</td></tr>\n",
       "<tr><td>train_ttpd_with_cv_17ec2f1e</td><td>TERMINATED</td><td>127.0.0.1:12825</td><td>[&#x27;proj_t_g&#x27;, &#x27;p_b6c0</td><td style=\"text-align: right;\">0.085112   </td><td>                     </td><td>lbfgs               </td><td style=\"text-align: right;\">        0.609705</td><td style=\"text-align: right;\">            1000</td><td style=\"text-align: right;\">  0.233161  </td><td>l2</td><td>lbfgs    </td><td style=\"text-align: right;\">           0.134051</td><td style=\"text-align: right;\">               1000</td><td>True        </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        0.153144</td><td style=\"text-align: right;\">  0.689655</td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">0.310345 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 18:22:03,606\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-10-10 18:22:03,612\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/flohop/ray_results/ttpd_optimization' in 0.0051s.\n",
      "2025-10-10 18:22:04,193\tINFO tune.py:1041 -- Total run time: 17.14 seconds (16.53 seconds for the tuning loop).\n",
      "2025-10-10 18:22:04,194\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2025-10-10 18:22:04,209\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_ttpd_with_cv_8606f3c5: FileNotFoundError('Could not fetch metrics for train_ttpd_with_cv_8606f3c5: both result.json and progress.csv were not found at /Users/flohop/ray_results/ttpd_optimization/train_ttpd_with_cv_8606f3c5_6_features=proj_t_g_proj_p_proj_t_p_inter,final_C=0.3164,penalty=l1,solver=liblinear,final_l1_ratio=0._2025-10-10_18-22-01')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Best Trial: train_ttpd_with_cv_6c5f04fe\n",
      "Best Accuracy: 0.9562\n",
      "Accuracy Std Dev: 0.0589\n",
      "\n",
      "Best Hyperparameters:\n",
      "--------------------------------------------------------------------------------\n",
      "  polarity_combo: {'penalty': 'l2', 'solver': 'liblinear'}\n",
      "  final_combo: {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "  polarity_C: 0.010894470780762364\n",
      "  polarity_l1_ratio: 0.7740809777807832\n",
      "  polarity_max_iter: 1000\n",
      "  final_C: 0.34455286384677336\n",
      "  final_max_iter: 2000\n",
      "  final_l1_ratio: 0.7021846107475395\n",
      "  use_scaler: False\n",
      "  features: ['proj_t_g', 'proj_p', 'proj_t_p_inter']\n",
      "\n",
      "================================================================================\n",
      "TOP 10 CONFIGURATIONS\n",
      "================================================================================\n",
      "          accuracy  config/polarity_C config/polarity_penalty config/final_penalty\n",
      "trial_id                                                                          \n",
      "6c5f04fe  0.956175           0.010894                      l2                   l1\n",
      "45c0bf76  0.950263           0.042351                      l2                   l1\n",
      "02e153fd  0.932290           2.046978                      l2                   l2\n",
      "7c461e90  0.931189           0.000930                      l1                 None\n",
      "17ec2f1e  0.689655           0.233161                      l2                 None\n",
      "8606f3c5       NaN                NaN                     NaN                  NaN\n",
      "\n",
      "================================================================================\n",
      "PARAMETER STATISTICS\n",
      "================================================================================\n",
      "\n",
      "polarity_C:\n",
      "                       mean  std  count\n",
      "config/polarity_C                      \n",
      "0.000930           0.931189  NaN      1\n",
      "0.010894           0.956175  NaN      1\n",
      "0.042351           0.950263  NaN      1\n",
      "0.233161           0.689655  NaN      1\n",
      "2.046978           0.932290  NaN      1\n",
      "\n",
      "polarity_penalty:\n",
      "                             mean       std  count\n",
      "config/polarity_penalty                           \n",
      "l1                       0.931189       NaN      1\n",
      "l2                       0.882096  0.128695      4\n",
      "\n",
      "final_penalty:\n",
      "                          mean       std  count\n",
      "config/final_penalty                           \n",
      "l1                    0.953219  0.004181      2\n",
      "l2                    0.932290       NaN      1\n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\n",
      "================================================================================\n",
      "\n",
      "Final Model Validation Accuracy: 0.7143\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature importance"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Add Scaler here\n",
    "config = {\n",
    "    \"polarity_C\": 1.0,                        # default regularization strength (unused here)\n",
    "    \"polarity_penalty\": \"l2\",                 # typical default, though your code doesn't use it\n",
    "    \"polarity_solver\": \"lbfgs\",               # stable for small feature counts\n",
    "    \"polarity_max_iter\": 1000,                # reasonable iteration cap\n",
    "\n",
    "\n",
    "\n",
    "    \"features\": [],\n",
    "\n",
    "    \"final_penalty\": None,                    # matches your LogisticRegression(penalty=None)\n",
    "    \"final_C\": None,                          # not used since penalty=None\n",
    "    \"final_solver\": \"lbfgs\",                  # sklearn default for multi-purpose use\n",
    "    \"final_max_iter\": 2000                    # matches your pattern of long convergence caps\n",
    "}\n",
    "\n",
    "features_config = [\n",
    "    [\"proj_t_g\", \"proj_p\", \"proj_t_p\", \"proj_t_p_inter\"],\n",
    "    [\"proj_t_g\", \"proj_t_p_inter\"],\n",
    "    [\"proj_t_g\", \"proj_p\"],\n",
    "    [\"proj_t_g\", \"proj_p\", \"proj_t_p\"],\n",
    "    [\"proj_t_g\", \"proj_p\", \"proj_t_p_inter\"],\n",
    "]\n",
    "\n",
    "acts_centered_train, acts_train, labels_train, polarities_train = collect_training_data(\n",
    "        train_sets, train_set_sizes, model_family, model_size, model_type, layer)\n",
    "\n",
    "for feature_config in features_config:\n",
    "    config[\"features\"] = feature_config\n",
    "\n",
    "\n",
    "\n",
    "    avg_coef, avg_norm_coef = get_average_coef(acts_centered_train, acts_train, labels_train, polarities_train, runs=10, config=config)\n",
    "\n",
    "    plot_lr_feature_importance(avg_coef, feature_names=feature_config)\n",
    "    plot_lr_feature_importance(avg_norm_coef, feature_names=feature_config, title=\"Feature Importance Norm. (|Coefficient|)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Polarity direction accuracy"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\", \"sp_en_trans_disj\",\n",
    "                \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "                \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\",\n",
    "                \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "val_set_sizes = dataset_sizes(val_sets)\n",
    "\n",
    "cv_train_sets = np.array(train_sets)\n",
    "cv_test_sets = np.array(val_sets)\n",
    "acts_centered, acts, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family,\n",
    "                                                                    model_size, model_type, layer)\n",
    "\n",
    "# Test set\n",
    "t_acts_centered, t_acts, t_labels, t_polarities = collect_training_data(cv_test_sets, dataset_sizes(val_sets), model_family, model_size, model_type, layer)\n",
    "\n",
    "measure_polarity_direction_lr(acts, polarities, t_acts, t_polarities)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unseen topics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compare TTPD, LR and CCS on topic-specific datasets\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "num_iter = 3\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter * len(train_sets)\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            indices = np.arange(0, 12, 2)\n",
    "            for i in indices:\n",
    "                cv_train_sets = np.delete(np.array(train_sets), [i, i+1], axis=0)\n",
    "                # load training data\n",
    "                acts_centered, acts, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family,\n",
    "                                                                                model_size, model_type, layer)\n",
    "\n",
    "                if probe_type in TTPD_CLASSES:\n",
    "                    probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "                elif probe_type == LRProbe:\n",
    "                    probe = LRProbe.from_data(acts, labels)\n",
    "                elif probe_type == CCSProbe:\n",
    "                    acts_affirm = acts[polarities == 1.0]\n",
    "                    acts_neg = acts[polarities == -1.0]\n",
    "                    labels_affirm = labels[polarities == 1.0]\n",
    "                    mean_affirm = torch.mean(acts_affirm, dim=0)\n",
    "                    mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                    acts_affirm = acts_affirm - mean_affirm\n",
    "                    acts_neg = acts_neg - mean_neg\n",
    "                    probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "                elif probe_type == MMProbe:\n",
    "                    probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "                # evaluate classification accuracy on held out datasets\n",
    "                dm = DataManager()\n",
    "                for j in range(0,2):\n",
    "                    dm.add_dataset(train_sets[i+j], model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                    acts, labels = dm.data[train_sets[i+j]]\n",
    "\n",
    "                    # classifier specific predictions\n",
    "                    if probe_type == CCSProbe:\n",
    "                        if j == 0:\n",
    "                            acts = acts - mean_affirm\n",
    "                        if j == 1:\n",
    "                            acts = acts - mean_neg\n",
    "                    predictions = probe.pred(acts)\n",
    "                    results[probe_type][train_sets[i+j]].append((predictions == labels).float().mean().item())\n",
    "                    pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "probes = [p for (t, p ) in ALL_PROBES]\n",
    "titles = [t for (t, p) in ALL_PROBES]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6), ncols=len(probes))\n",
    "\n",
    "if len(probes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "\n",
    "for t, (ax, key) in enumerate(zip(axes, probes)):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in train_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in train_sets]\n",
    "\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}',\n",
    "                    ha='center', va='center', fontsize=13)\n",
    "\n",
    "    ax.set_yticks(range(len(train_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=12)\n",
    "\n",
    "# y tick labels only on first subplot\n",
    "axes[0].set_yticklabels(train_sets, fontsize=12)\n",
    "for ax in axes[1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6, location=\"right\")\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation to logical conjunctions and disjunctions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compare TTPD, LR, CCS and MM on logical conjunctions and disjunctions\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\",\"sp_en_trans_disj\",\n",
    "             \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "               \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\",\n",
    "            \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n",
    "\n",
    "\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                             model_type, layer)\n",
    "\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set] # retrieve the activations and labels that were just added to the DM\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts) # one prediction per example. 0 if we think its a lie, 1 if we predicte its true\n",
    "\n",
    "                # compare prediction with ground truth labels and average it\n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "probes = [p for (t, p ) in ALL_PROBES]\n",
    "titles = [t for (t, p) in ALL_PROBES]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6), ncols=len(probes))\n",
    "\n",
    "if len(probes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "\n",
    "for t, (ax, key) in enumerate(zip(axes, probes)):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in val_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in val_sets]\n",
    "\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}',\n",
    "                    ha='center', va='center', fontsize=13)\n",
    "\n",
    "    ax.set_yticks(range(len(val_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=12)\n",
    "\n",
    "# y tick labels only on first subplot\n",
    "axes[0].set_yticklabels(val_sets, fontsize=12)\n",
    "for ax in axes[1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6, location='right')\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation to German statements"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compare TTPD, LR, CCS and MM on statements translated to german\n",
    "val_sets = [\"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "                  \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\"]\n",
    "\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                                           model_type, layer)\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                \n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probes = [p for (t, p ) in ALL_PROBES]\n",
    "titles = [t for (t, p) in ALL_PROBES]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6), ncols=len(probes))\n",
    "\n",
    "\n",
    "if len(probes) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for t, (ax, key) in enumerate(zip(axes, probes)):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in val_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in val_sets]\n",
    "\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}',\n",
    "                    ha='center', va='center', fontsize=13)\n",
    "\n",
    "    ax.set_yticks(range(len(val_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=12)\n",
    "\n",
    "# y tick labels only on first subplot\n",
    "axes[0].set_yticklabels(val_sets, fontsize=12)\n",
    "for ax in axes[1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.6, location='right')\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying generalisation to Conjunctions, Disjunctions and German statements in one table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the validation sets and the probe types\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\",\"sp_en_trans_disj\",\n",
    "             \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "               \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\", \"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "                  \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\",\n",
    "            \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: defaultdict(list) for t in probe_types}\n",
    "num_iter = 20\n",
    "\n",
    "TTPD_CLASSES = [v for (k, v) in TTPD_TYPES]\n",
    "\n",
    "# Training and evaluating classifiers\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar:\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                                           model_type, layer)\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the groups\n",
    "groups = {\n",
    "    'Conjunctions': [dataset for dataset in val_sets if dataset.endswith('_conj')],\n",
    "    'Disjunctions': [dataset for dataset in val_sets if dataset.endswith('_disj')],\n",
    "    'Affirmative German': [dataset for dataset in val_sets if dataset.endswith('_de') and not dataset.startswith('neg_')],\n",
    "    'Negated German': [dataset for dataset in val_sets if dataset.startswith('neg_') and dataset.endswith('_de')],\n",
    "    'common_claim_true_false': ['common_claim_true_false'],\n",
    "    'counterfact_true_false': ['counterfact_true_false']\n",
    "}\n",
    "\n",
    "# Initialize group results\n",
    "group_results = {probe_type: {group_name: [] for group_name in groups} for probe_type in probe_types}\n",
    "\n",
    "# Process results to compute mean accuracies per group per classifier\n",
    "for probe_type in probe_types:\n",
    "    for n in range(num_iter):\n",
    "        for group_name, group_datasets in groups.items():\n",
    "            accuracies = []\n",
    "            for dataset in group_datasets:\n",
    "                accuracy = results[probe_type][dataset][n]\n",
    "                accuracies.append(accuracy)\n",
    "            mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "            group_results[probe_type][group_name].append(mean_accuracy)\n",
    "\n",
    "# Compute statistics\n",
    "stat_group_results = {probe_type: {'mean': {}, 'std': {}} for probe_type in probe_types}\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    for group_name in groups:\n",
    "        accuracies = group_results[probe_type][group_name]\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        stat_group_results[probe_type]['mean'][group_name] = mean_accuracy\n",
    "        stat_group_results[probe_type]['std'][group_name] = std_accuracy\n",
    "\n",
    "# Map probe types to classifier names\n",
    "\n",
    "probe_type_to_name = {probe:name for (name, probe) in ALL_PROBES}\n",
    "\n",
    "# probe_type_to_name = {\n",
    "#     TTPD: 'TTPD',\n",
    "#     TTPD4d: \"TTPD4d\",\n",
    "#     TTPD3dTp: \"TTPD3dTp\",\n",
    "#     TTPD3dTpInv: \"TTPD3dTpInv\",\n",
    "#     LRProbe: 'LR',\n",
    "#     CCSProbe: 'CCS',\n",
    "#     MMProbe: 'MM'\n",
    "# }\n",
    "\n",
    "# Create DataFrames for mean accuracies and standard deviations\n",
    "group_names = ['Conjunctions', 'Disjunctions', 'Affirmative German', 'Negated German', 'common_claim_true_false', 'counterfact_true_false']\n",
    "classifier_names = [n for (n, _) in ALL_PROBES]\n",
    "# classifier_names = ['TTPD', \"TTPD4d\", \"TTPD3dTp\", \"TTPD3dTpInv\", 'LR', 'CCS', 'MM']\n",
    "\n",
    "mean_df = pd.DataFrame(index=group_names, columns=classifier_names)\n",
    "std_df = pd.DataFrame(index=group_names, columns=classifier_names)\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    classifier_name = probe_type_to_name[probe_type]\n",
    "    for group_name in group_names:\n",
    "        mean_accuracy = stat_group_results[probe_type]['mean'][group_name]\n",
    "        std_accuracy = stat_group_results[probe_type]['std'][group_name]\n",
    "        mean_df.loc[group_name, classifier_name] = mean_accuracy\n",
    "        std_df.loc[group_name, classifier_name] = std_accuracy\n",
    "\n",
    "num_classifiers = len(classifier_names)\n",
    "fig, axes = plt.subplots(figsize=(2.5*num_classifiers, 6), ncols=num_classifiers)\n",
    "\n",
    "for idx, classifier_name in enumerate(classifier_names):\n",
    "    ax = axes[idx]\n",
    "    mean_values = mean_df[classifier_name].values.astype(float)\n",
    "    std_values = std_df[classifier_name].values.astype(float)\n",
    "\n",
    "    # Create heatmap with a single column\n",
    "    im = ax.imshow(mean_values[:, np.newaxis], vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "\n",
    "    # Annotate the heatmap\n",
    "    for i in range(len(group_names)):\n",
    "        mean_accuracy = mean_values[i]\n",
    "        std_accuracy = std_values[i]\n",
    "        ax.text(0, i, f'{round(mean_accuracy * 100):2d} ± {round(std_accuracy * 100):2d}',\n",
    "                ha='center', va='center', fontsize=14)\n",
    "\n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    if idx == 0:\n",
    "        ax.set_yticks(np.arange(len(group_names)))\n",
    "        ax.set_yticklabels(group_names, fontsize=14)\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    ax.set_title(classifier_name, fontsize=15)\n",
    "\n",
    "# Add colorbar on the right\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), location='right', shrink=0.8)\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Classification Accuracies\", fontsize=17)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world scenarios / lies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probe_types = [t for (name, t) in ALL_PROBES]\n",
    "results = {t: [] for t in probe_types}\n",
    "num_iter = 50\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family,\n",
    "                                                                                           model_size, model_type,layer)\n",
    "            if probe_type in TTPD_CLASSES:\n",
    "                probe = probe_type.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on real world scenarios\n",
    "            dm = DataManager()\n",
    "            real_world_dataset = \"real_world_scenarios/all_unambiguous_replies\"\n",
    "            dm.add_dataset(real_world_dataset, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "            acts, labels = dm.data[real_world_dataset]\n",
    "            \n",
    "            # classifier specific predictions\n",
    "            if probe_type == CCSProbe:\n",
    "                acts = acts - (mean_affirm + mean_neg)/2\n",
    "\n",
    "            predictions = probe.pred(acts)\n",
    "            results[probe_type].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    mean = np.mean(results[probe_type])\n",
    "    std = np.std(results[probe_type])\n",
    "    print(f\"{probe_type.__name__}:\")\n",
    "    print(f\"  Mean Accuracy: {mean*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation: {std*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
